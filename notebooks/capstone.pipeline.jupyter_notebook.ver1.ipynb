{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "Standard Imports", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Requirement already satisfied: sklearn-pandas in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (1.8.0)\nRequirement already satisfied: numpy>=1.6.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from sklearn-pandas) (1.13.3)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from sklearn-pandas) (1.0.0)\nRequirement already satisfied: pandas>=0.11.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from sklearn-pandas) (0.21.0)\nRequirement already satisfied: scikit-learn>=0.15.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from sklearn-pandas) (0.19.1)\nRequirement already satisfied: python-dateutil>=2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas>=0.11.0->sklearn-pandas) (2.6.1)\nRequirement already satisfied: pytz>=2011k in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pandas>=0.11.0->sklearn-pandas) (2018.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python-dateutil>=2->pandas>=0.11.0->sklearn-pandas) (1.11.0)\n\u001b[31mtensorflow 1.3.0 requires tensorflow-tensorboard<0.2.0,>=0.1.0, which is not installed.\u001b[0m\n"
                }
            ], 
            "source": "!pip install sklearn-pandas"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import datetime\nimport gc\nimport types\nimport random\nimport re\nimport sys\nimport pkg_resources\n\nimport types\nfrom botocore.client import Config\nimport ibm_boto3\n\nimport matplotlib.pyplot as plt # data visualization\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pickle\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GroupShuffleSplit, GroupKFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelBinarizer, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn_pandas import DataFrameMapper, gen_features, FunctionTransformer\n\n%matplotlib inline"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "gc==Python BuiltIn\nsys==Python BuiltIn\nscikit-learn==0.19.1\npandas==0.21.0\nnumpy==1.13.3\nmatplotlib==2.1.0\nbotocore==1.7.20\ntypes==unknown\nrandom==unknown\nibm_boto3==unknown\npickle==unknown\nre==unknown\npkg_resources==unknown\ndatetime==unknown\nsklearn_pandas==unknown\n"
                }
            ], 
            "source": "# show versions of packages\n# adopted from https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook\n\ndef get_imports():\n    for name, val in globals().items():\n        if isinstance(val, types.ModuleType):\n            # Split ensures you get root package, \n            # not just imported function\n            name = val.__name__.split(\".\")[0]\n        elif isinstance(val, type):\n            name = val.__module__.split(\".\")[0]\n        # Some packages are weird and have different\n        # imported names vs. system/pip names. Unfortunately,\n        # there is no systematic way to get pip names from\n        # a package's imported name. You'll have to add\n        # exceptions to this list manually!\n        poorly_named_packages = {\n            \"sklearn\": \"scikit-learn\"\n        }\n        if name in poorly_named_packages.keys():\n            name = poorly_named_packages[name]\n        yield name.lower()\nimports = list(set(get_imports()))\n\n# The only way I found to get the version of the root package\n# from only the name of the package is to cross-check the names \n# of installed packages vs. imported packages\nmodules = []\nfor m in sys.builtin_module_names:\n    if m.lower() in imports and m !='builtins':\n        modules.append((m,'Python BuiltIn'))\n        imports.remove(m.lower())\n\nfor m in pkg_resources.working_set:\n    if m.project_name.lower() in imports and m.project_name!=\"pip\":\n        modules.append((m.project_name, m.version))\n        imports.remove(m.project_name.lower())\n\nfor m in sys.modules:\n    if m.lower() in imports and m !='builtins':\n        modules.append((m,'unknown'))\n\n\nfor r in modules:\n    print(\"{}=={}\".format(*r))"
        }, 
        {
            "source": "Utility Functions", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def preprocess(df):\n    \n    # Fill N/A values\n    df.season_holidayed_code.fillna(-1, inplace = True)\n    df.state_code_residence.fillna(-1, inplace = True)\n    \n    # Parse dates\n    for col in ['checkout_date', 'checkin_date', 'booking_date']:\n        df[col] = df[col].apply(lambda x: pd.datetime.strptime(x, '%d/%m/%y'))\n        \n    # Add datetime features\n    df['booking_in_advance'] = (df['checkin_date'] - df['booking_date']).dt.days\n    df['days_stayed'] = (df['checkout_date'] - df['checkin_date']).dt.days\n    \n    # Apply add_datepart function to datetime features\n    for col in ['checkout_date', 'checkin_date', 'booking_date']:\n        # add date features\n        add_datepart(df, col, drop=True)\n    \n    # Add other features\n    df['n_people'] = df['numberofadults'] + df['numberofchildren']\n    \n    return df"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def add_datepart(df, fldname, drop=True, time=False, errors=\"raise\"):\n    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n    the information from the date. This applies changes inplace.\n    Parameters:\n    -----------\n    df: A pandas data frame. df gain several new columns.\n    fldname: A string that is the name of the date column you wish to expand.\n        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n    drop: If true then the original date column will be removed.\n    time: If true time features: Hour, Minute, Second will be added.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })\n    >>> df\n        A\n    0   2000-03-11\n    1   2000-03-12\n    2   2000-03-13\n    >>> add_datepart(df, 'A')\n    >>> df\n        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n    \"\"\"\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n    if drop: df.drop(fldname, axis=1, inplace=True)"
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "class ColumnSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n\n        try:\n            return X[self.columns]\n        except KeyError:\n            cols_error = list(set(self.columns) - set(X.columns))\n            raise KeyError(\"The DataFrame does not include the columns: %s\" % cols_error)"
        }, 
        {
            "source": "Access to IBM Cloud Object Storage", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "source": "Load Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "CPU times: user 2.08 s, sys: 476 ms, total: 2.56 s\nWall time: 3.41 s\n"
                }
            ], 
            "source": "%%time\n# define the datetime data types dictionary\ndate_feats = ['checkout_date', 'checkin_date', 'booking_date']\n\n# load the data and take a look at first 5 rows\ndf = pd.read_csv(body)"
        }, 
        {
            "source": "Pre-process the data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "CPU times: user 15.8 s, sys: 532 ms, total: 16.3 s\nWall time: 17.1 s\n"
                }
            ], 
            "source": "%%time\ndf = preprocess(df)"
        }, 
        {
            "source": "Define the data types.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dtypes_dict = {}\n\ncategory_feats = df.columns[df.dtypes=='object'].tolist() + \\\n['room_type_booked_code', 'booking_type_code', 'state_code_resort', 'channel_code', \n 'main_product_code', 'persontravellingid', 'resort_region_code', 'resort_type_code']\n\ncategory_feats = [x for x in category_feats if x not in date_feats + ['memberid', 'reservation_id']]\n\nbin_feats = []\n\nfor col in df.columns:\n    if len(df[col].unique()) == 2:\n        bin_feats.append(col)\n\ndtypes_dict = {\n    \"date_feats\": date_feats, \n    \"category_feats\": category_feats,\n    \"bin_feats\": bin_feats}\n\n\ndtypes_dict['num_feats'] = [x for x in df.columns if x not in sum(dtypes_dict.values(), ['reservation_id', 'memberid', 'amount_spent_per_room_night_scaled'])]"
        }, 
        {
            "source": "Create and fit sklearn pipeline", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "category_feats = gen_features(\n    columns=dtypes_dict['category_feats']+dtypes_dict['bin_feats'], \n    classes=[LabelBinarizer])\n\nnum_feats = gen_features(\n    columns=[[col] for col in dtypes_dict['num_feats']], \n    classes=[StandardScaler])\n\nmapper = DataFrameMapper(category_feats+num_feats, df_out=True, default=None)\n\ncolumns = [c for c in df.columns if c not in ['memberid', 'reservation_id', 'amount_spent_per_room_night_scaled']]\ncs = ColumnSelector(columns=columns)"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "pipeline = Pipeline([\n    ('column_dropper', cs),\n    ('mapper_transformer', mapper), \n    ('ridge_model', Ridge(alpha=1, random_state=42))\n])"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "CPU times: user 28.5 s, sys: 35.6 s, total: 1min 4s\nWall time: 1min 5s\n"
                }, 
                {
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Pipeline(memory=None,\n     steps=[('column_dropper', ColumnSelector(columns=['channel_code', 'main_product_code', 'numberofadults', 'numberofchildren', 'persontravellingid', 'resort_region_code', 'resort_type_code', 'room_type_booked_code', 'roomnights', 'season_holidayed_code', 'state_code_residence', 'state_code_resort', 't... fit_intercept=True, max_iter=None,\n   normalize=False, random_state=42, solver='auto', tol=0.001))])"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "%%time\npipeline.fit(df.drop(['amount_spent_per_room_night_scaled'], axis=1), df['amount_spent_per_room_night_scaled'])"
        }, 
        {
            "source": "Save pipeline to pickle", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with open('pipeline.pickle', 'wb') as pkl:\n    pickle.dump(pipeline, pkl, protocol=pickle.HIGHEST_PROTOCOL)\n    \ncos.upload_file(Filename='pipeline.pickle', Bucket=credentials['BUCKET'], Key='pipeline.pickle')"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}